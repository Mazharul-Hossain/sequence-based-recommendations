{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def command_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-f', dest='filename', help='Input file', required=True, type=str)\n",
    "    parser.add_argument('--columns',\n",
    "                        help='Order of the columns in the file (eg: \"uirt\"), u for user, i for item, t for timestamp, '\n",
    "                             'r for rating. If r is not present a default rating of 1 is given to all interaction. If '\n",
    "                             't is not present interactions are assumed to be in chronological order. Extra columns '\n",
    "                             'are ignored. Default: uit',\n",
    "                        default=\"uit\", type=str)\n",
    "    parser.add_argument('--sep',\n",
    "                        help='Separator between the column. If unspecified pandas will try to guess the separator',\n",
    "                        default=\"\\t\", type=str)\n",
    "    parser.add_argument('--min_user_activity',\n",
    "                        help='Users with less interactions than this will be removed from the dataset. Default: 2',\n",
    "                        default=2, type=int)\n",
    "    parser.add_argument('--min_item_pop',\n",
    "                        help='Items with less interactions than this will be removed from the dataset. Default: 5',\n",
    "                        default=5, type=int)\n",
    "    parser.add_argument('--val_size',\n",
    "                        help='Number of users to put in the validation set. If in (0,1) it will be interpreted as the '\n",
    "                             'fraction of total number of users. Default: 0.1',\n",
    "                        default=0.1, type=float)\n",
    "    parser.add_argument('--test_size',\n",
    "                        help='Number of users to put in the test set. If in (0,1) it will be interpreted as the '\n",
    "                             'fraction of total number of users. Default: 0.1',\n",
    "                        default=0.1, type=float)\n",
    "    parser.add_argument('--seed', help='Seed for the random train/val/test split', default=1, type=int)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.dirname = os.path.dirname(os.path.abspath(args.filename)) + \"/\"\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn_user(dirname):\n",
    "    \"\"\"\n",
    "    Ask user if he's sure to create files in that directory.\n",
    "    \"\"\"\n",
    "    print('This program will create a lot of files and directories in ' + dirname)\n",
    "    answer = input('Are you sure that you want to do that ? [y/n]')\n",
    "    if answer != \"y\" or answer != \"Y\":\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs(dirname):\n",
    "    if not os.path.exists(dirname + \"data\"):\n",
    "        os.makedirs(dirname + \"data\")\n",
    "\n",
    "    if not os.path.exists(dirname + \"models\"):\n",
    "        os.makedirs(dirname + \"models\")\n",
    "\n",
    "    if not os.path.exists(dirname + \"results\"):\n",
    "        os.makedirs(dirname + \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, columns, separator):\n",
    "    \"\"\"\n",
    "    Load the data from filename and sort it according to timestamp.\n",
    "    Returns a dataframe with 3 columns: user_id, item_id, rating\n",
    "    \"\"\"\n",
    "    def timestamp_to_date_converter(t):\n",
    "        return datetime.datetime.fromtimestamp(t).strftime(\"%Y-%m-%d, %I:%M:%S\")\n",
    "\n",
    "    print('Load data...')\n",
    "    data = pd.read_csv(filename)\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    replace_columns = {}\n",
    "    for k, v in zip(data.columns, list(columns)):\n",
    "        replace_columns[k] = v\n",
    "    data.rename(columns = replace_columns, inplace = True)\n",
    "    print(data.dtypes)\n",
    "\n",
    "    if 'r' not in columns:\n",
    "        # Add a column of default ratings\n",
    "        data['r'] = 1\n",
    "\n",
    "    if 't' in columns:        \n",
    "        # sort according to the timestamp column\n",
    "        if data['t'].dtype == np.int64:  # probably a timestamp\n",
    "            data['t'] = pd.to_datetime(data['t'], unit='s')\n",
    "        else:\n",
    "            data['t'] = pd.to_datetime(data['t'])\n",
    "        \n",
    "        print('Sort data in chronological order...')\n",
    "        data.sort_values('t', inplace=True)\n",
    "\n",
    "    print(data.dtypes)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_elements(data, min_user_activity, min_item_popularity):\n",
    "    \"\"\"\n",
    "    Removes user and items that appears in too few interactions.\n",
    "    min_user_activity is the minimum number of interaction that a user should have.\n",
    "    min_item_popularity is the minimum number of interaction that an item should have.\n",
    "    NB: the constraint on item might not be strictly satisfied because rare users and items are removed in alternate,\n",
    "    and the last removal of inactive users might create new rare items.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Remove inactive users and rare items...')\n",
    "\n",
    "#     # Remove inactive users a first time\n",
    "#     user_activity = data.groupby('u').size()\n",
    "#     data = data[np.in1d(data.u, user_activity[user_activity >= min_user_activity].index)]\n",
    "    \n",
    "    # Remove unpopular items\n",
    "    item_popularity = data.groupby('i').size()\n",
    "    data = data[np.in1d(data.i, item_popularity[item_popularity >= min_item_popularity].index)]\n",
    "    \n",
    "    # Remove users that might have passed below the activity threshold due to the removal of rare items\n",
    "    user_activity = data.groupby('u').size()\n",
    "    data = data[np.in1d(data.u, user_activity[user_activity >= min_user_activity].index)]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index_mapping(data, dirname, separator=\"\\t\"):\n",
    "    \"\"\"\n",
    "    Save the mapping of original user and item ids to numerical consecutive ids in dirname.\n",
    "    NB: some users and items might have been removed in previous steps and will therefore not appear in the mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pandas categorical type will create the numerical ids we want\n",
    "    print('Map original users and items ids to consecutive numerical ids...')\n",
    "    data['u_original'] = data['u'].astype('category')\n",
    "    data['i_original'] = data['i'].astype('category')\n",
    "    data['u'] = data['u_original'].cat.codes\n",
    "    data['i'] = data['i_original'].cat.codes\n",
    "\n",
    "    print('Save ids mapping to file...')\n",
    "    user_mapping = pd.DataFrame({'original_id': data['u_original'], 'new_id': data['u']})\n",
    "    user_mapping.sort_values('original_id', inplace=True)\n",
    "    user_mapping.drop_duplicates(subset='original_id', inplace=True)\n",
    "    user_mapping.to_csv(dirname + \"data/user_id_mapping.txt\", sep=separator, index=False)\n",
    "\n",
    "    item_mapping = pd.DataFrame({'original_id': data['i_original'], 'new_id': data['i']})\n",
    "    item_mapping.sort_values('original_id', inplace=True)\n",
    "    item_mapping.drop_duplicates(subset='original_id', inplace=True)\n",
    "    item_mapping.to_csv(dirname + \"data/item_id_mapping.txt\", sep=separator, index=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, nb_val_users, nb_test_users, dirname):\n",
    "    \"\"\"\n",
    "    Splits the data set into training, validation and test sets.\n",
    "    Each user is in one and only one set.\n",
    "    nb_val_users is the number of users to put in the validation set.\n",
    "    nb_test_users is the number of users to put in the test set.\n",
    "    \"\"\"\n",
    "    nb_users = data['u'].nunique()\n",
    "\n",
    "    # check if nb_val_user is specified as a fraction\n",
    "    if nb_val_users < 1:\n",
    "        nb_val_users = round(nb_val_users * nb_users)\n",
    "    if nb_test_users < 1:\n",
    "        nb_test_users = round(nb_test_users * nb_users)\n",
    "    nb_test_users = int(nb_test_users)\n",
    "    nb_val_users = int(nb_val_users)\n",
    "\n",
    "    if nb_users <= nb_val_users + nb_test_users:\n",
    "        raise ValueError('Not enough users in the dataset: choose less users for validation and test splits')\n",
    "\n",
    "    def extract_n_users(df, n, test=False):\n",
    "        if test:\n",
    "            temp_df = df.loc[df.t > '2017-01-01']\n",
    "            users_ids = np.random.choice(temp_df['u'].unique(), n)\n",
    "        else:\n",
    "            users_ids = np.random.choice(df['u'].unique(), n)\n",
    "        \n",
    "        n_set = df[df['u'].isin(users_ids)]\n",
    "        remain_set = df.drop(n_set.index)\n",
    "        return n_set, remain_set\n",
    "\n",
    "    print('Split data into training, validation and test sets...')\n",
    "    test_set, tmp_set = extract_n_users(data, nb_test_users, test=True)\n",
    "    val_set, train_set = extract_n_users(tmp_set, nb_val_users)\n",
    "\n",
    "    print('Save training, validation and test sets in the triplets format...')\n",
    "    train_set.to_csv(dirname + \"data/train_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "    val_set.to_csv(dirname + \"data/val_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "    test_set.to_csv(dirname + \"data/test_set_triplets\", sep=\"\\t\", columns=['u', 'i', 'r'], index=False, header=False)\n",
    "\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sequences(data, half=False):\n",
    "    \"\"\"\n",
    "    Generates sequences of user actions from data.\n",
    "    each sequence has the format [user_id, first_item_id, first_item_rating, 2nd_item_id, 2nd_item_rating, ...].\n",
    "    If half is True, cut the sequences to half their true length (useful to produce the extended training set).\n",
    "    \"\"\"\n",
    "    data = data.sort_values('u', kind=\"mergesort\")  # Mergesort is stable and keeps the time ordering\n",
    "    seq = []\n",
    "    prev_id = -1\n",
    "    for u, i, r in zip(data['u'], data['i'], data['r']):\n",
    "        if u != prev_id:\n",
    "            if len(seq) > 3:\n",
    "                if half:\n",
    "                    seq = seq[:1 + 2 * int((len(seq) - 1) / 4)]\n",
    "                yield seq\n",
    "            prev_id = u\n",
    "            seq = [u]\n",
    "        seq.extend([i, r])\n",
    "    if half:\n",
    "        seq = seq[:1 + 2 * int((len(seq) - 1) / 4)]\n",
    "    yield seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequence_format(train_set, val_set, test_set, dirname):\n",
    "    \"\"\"\n",
    "    Convert the train/validation/test sets in the sequence format and save them.\n",
    "    Also create the extended training sequences, which contains the first half of the sequences\n",
    "    of users in the validation and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Save the training set in the sequences format...')\n",
    "    with open(dirname + \"data/train_set_sequences\", \"w\") as f:\n",
    "        for s in gen_sequences(train_set):\n",
    "            f.write(' '.join(map(str, s)) + \"\\n\")\n",
    "\n",
    "    print('Save the validation set in the sequences format...')\n",
    "    with open(dirname + \"data/val_set_sequences\", \"w\") as f:\n",
    "        for s in gen_sequences(val_set):\n",
    "            f.write(' '.join(map(str, s)) + \"\\n\")\n",
    "\n",
    "    print('Save the test set in the sequences format...')\n",
    "    with open(dirname + \"data/test_set_sequences\", \"w\") as f:\n",
    "        for s in gen_sequences(test_set):\n",
    "            f.write(' '.join(map(str, s)) + \"\\n\")\n",
    "\n",
    "    # sequences+ contains all the sequences of train_set_sequences plus half the sequences of val and test sets\n",
    "    print('Save the extended training set in the sequences format...')\n",
    "    copyfile(dirname + \"data/train_set_sequences\", dirname + \"data/train_set_sequences+\")\n",
    "    with open(dirname + \"data/train_set_sequences+\", \"a\") as f:\n",
    "        for s in gen_sequences(val_set, half=True):\n",
    "            f.write(' '.join(map(str, s)) + \"\\n\")\n",
    "        for s in gen_sequences(test_set, half=True):\n",
    "            f.write(' '.join(map(str, s)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_stats(data, train_set, val_set, test_set, dirname):\n",
    "    print('Save stats...')\n",
    "\n",
    "    def _get_stats(df):\n",
    "        return \" \\t \".join(map(str, [df['u'].nunique(), df['i'].nunique(), len(df.index), df.groupby('u').size().max()]))\n",
    "\n",
    "    with open(dirname + \"data/stats\", \"w\") as f:\n",
    "        f.write(\"set \\t n_users \\t n_items \\t n_interactions \\t longest_sequence\\n\")\n",
    "        f.write(\"Full \\t \" + _get_stats(data) + \"\\n\")\n",
    "        f.write(\"Train \\t \" + _get_stats(train_set) + \"\\n\")\n",
    "        f.write(\"Val \\t \" + _get_stats(val_set) + \"\\n\")\n",
    "        f.write(\"Test \\t \" + _get_stats(test_set) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_readme(dirname, val_set, test_set):\n",
    "    print('Save readme...')\n",
    "    data_readme = '''The following files were automatically generated by preprocess.py\n",
    "\n",
    "                user_id_mapping\n",
    "                    mapping between the users ids in the original dataset and the new users ids.\n",
    "                    the first column contains the new id and the second the original id.\n",
    "                    Inactive users might have been deleted from the original, and they will therefore not appear in the \n",
    "                    id mapping.\n",
    "            \n",
    "                item_id_mapping\n",
    "                    Idem for item ids.\n",
    "            \n",
    "                train_set_triplets\n",
    "                    Training set in the triplets format.\n",
    "                    Each line is a user item interaction in the form (user_id, item_id, rating). \n",
    "                    Interactions are listed in chronological order.\n",
    "            \n",
    "                train_set_sequences\n",
    "                    Training set in the sequence format.\n",
    "                    Each line contains all the interactions of a user in the form (user_id, first_item_id, \n",
    "                    first_rating, 2nd_item_id, 2nd_rating, ...).\n",
    "            \n",
    "                train_set_sequences+\n",
    "                    Extended training set in the sequence format.\n",
    "                    The extended training set contains all the training set plus the first half of the \n",
    "                    interactions of each users in the validation and testing set.\n",
    "            \n",
    "                val_set_triplets\n",
    "                    Validation set in the triplets format\n",
    "            \n",
    "                val_set_triplets\n",
    "                    Validation set in the sequence format\n",
    "            \n",
    "                test_set_triplets\n",
    "                    Test set in the triplets format\n",
    "            \n",
    "                test_set_triplets\n",
    "                    Test set in the sequence format\n",
    "            \n",
    "                stats\n",
    "                    Contains some information about the dataset.\n",
    "            \n",
    "                The training, validation and test sets are obtain by randomly partitioning the users and \n",
    "                all their interactions into 3 sets.\n",
    "                The validation set contains {n_val} users, the test_set {n_test} users \n",
    "                and the train set all the other users.\n",
    "            \n",
    "                '''.format(n_val=str(val_set['u'].nunique()), n_test=str(test_set['u'].nunique()))\n",
    "\n",
    "    results_readme = '''The format of the results file is the following\n",
    "                    Each line correspond to one model, with the fields being:\n",
    "                        Number of epochs\n",
    "                        precision\n",
    "                        sps\n",
    "                        user coverage\n",
    "                        number of unique items in the test set\n",
    "                        number of unique items in the recommendations\n",
    "                        number of unique items in the successful recommendations\n",
    "                        number of unique items in the short-term test set (when the goal is to predict \n",
    "                        precisely the next item)\n",
    "                        number of unique items in the successful short-term recommendations\n",
    "                        recall \n",
    "                        NDCG\n",
    "                    NB: all the metrics are computed \"@10\"\n",
    "                    '''\n",
    "\n",
    "    with open(dirname + \"data/README\", \"w\") as f:\n",
    "        f.write(data_readme)\n",
    "    with open(dirname + \"results/README\", \"w\") as f:\n",
    "        f.write(results_readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(columns='uirt', dirname='D:\\\\UofMemphis\\\\Fall-20\\\\COMP 8150\\\\Project\\\\sequence-based-recommendations\\\\notebooks\\\\datasets/', filename='datasets/ratings.csv', min_item_pop=5, min_user_activity=2, seed=1, sep=',', test_size=0.1, val_size=0.1)\n",
      "Load data...\n",
      "userId         int64\n",
      "movieId        int64\n",
      "rating       float64\n",
      "timestamp      int64\n",
      "dtype: object\n",
      "u      int64\n",
      "i      int64\n",
      "r    float64\n",
      "t      int64\n",
      "dtype: object\n",
      "Sort data in chronological order...\n",
      "u             int64\n",
      "i             int64\n",
      "r           float64\n",
      "t    datetime64[ns]\n",
      "dtype: object\n",
      "         u       i    r                   t\n",
      "69524  448    6427  4.0 2004-04-18 09:58:23\n",
      "36317  247  134130  4.0 2016-07-04 14:55:02\n",
      "29612  202    2115  3.0 2000-11-22 20:00:59\n",
      "85008  552    2501  3.5 2005-03-30 02:57:39\n",
      "75517  477     922  4.5 2009-06-15 01:40:56\n",
      "Before remove_rare_elements: (100836, 4)\n",
      "Remove inactive users and rare items...\n",
      "After remove_rare_elements: (90274, 4)\n",
      "Map original users and items ids to consecutive numerical ids...\n",
      "Save ids mapping to file...\n",
      "Split data into training, validation and test sets...\n",
      "Save training, validation and test sets in the triplets format...\n",
      "Save the training set in the sequences format...\n",
      "Save the validation set in the sequences format...\n",
      "Save the test set in the sequences format...\n",
      "Save the extended training set in the sequences format...\n",
      "Save stats...\n",
      "Save readme...\n",
      "Data ready!\n",
      "         u    i    r                   t u_original i_original\n",
      "66719  428  369  5.0 1996-03-29 18:36:55        429        595\n",
      "66716  428  363  5.0 1996-03-29 18:36:55        429        588\n",
      "66717  428  365  5.0 1996-03-29 18:36:55        429        590\n",
      "66718  428  366  5.0 1996-03-29 18:36:55        429        592\n",
      "66712  428  275  3.0 1996-03-29 18:36:55        429        432\n",
      "66711  428  267  4.0 1996-03-29 18:36:55        429        421\n",
      "66710  428  266  2.0 1996-03-29 18:36:55        429        420\n",
      "66681  428  145  3.0 1996-03-29 18:36:55        429        227\n",
      "66680  428  144  4.0 1996-03-29 18:36:55        429        225\n",
      "66679  428  141  4.0 1996-03-29 18:36:55        429        222\n"
     ]
    }
   ],
   "source": [
    "sys.argv.extend(['-f', 'datasets/ratings.csv', '--columns', 'uirt', '--sep', ','])\n",
    "args = command_parser()\n",
    "print(args)\n",
    "\n",
    "np.random.seed(seed=args.seed)\n",
    "\n",
    "create_dirs(args.dirname)\n",
    "\n",
    "data = load_data(args.filename, args.columns, args.sep)\n",
    "print(data.sample(5))\n",
    "\n",
    "print(\"Before remove_rare_elements:\", data.shape)\n",
    "data = remove_rare_elements(data, args.min_user_activity, args.min_item_pop)\n",
    "print(\"After remove_rare_elements:\", data.shape)\n",
    "\n",
    "data = save_index_mapping(data, args.dirname)\n",
    "\n",
    "train_set, val_set, test_set = split_data(data, args.val_size, args.test_size, args.dirname)\n",
    "make_sequence_format(train_set, val_set, test_set, args.dirname)\n",
    "\n",
    "save_data_stats(data, train_set, val_set, test_set, args.dirname)\n",
    "make_readme(args.dirname, val_set, test_set)\n",
    "\n",
    "print('Data ready!')\n",
    "print(data.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
